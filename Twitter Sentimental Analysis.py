# -*- coding: utf-8 -*-
"""TSA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vwq4W4wftdrcddD2i3SBwQ9GeQSYOOzS

**Importing Libraries**
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

tweets_df = pd.read_csv('twitter.csv')
tweets_df

# droping id column
tweets_df.reset_index(drop = True).head()

"""**Explotary dataset**"""

sns.heatmap(tweets_df.isnull(), yticklabels = False, cbar = False, cmap="Blues")
tweets_df.hist(bins = 30, figsize = (13,5), color = 'r')

sns.countplot(tweets_df['label'], label = "Count")

tweets_df['length'] =tweets_df['tweet'].apply(len)

tweets_df['length'].plot(bins=100, kind='hist') 
tweets_df.describe()

# Let's see the message with mean length 
tweets_df[tweets_df['length'] == 84]['tweet'].iloc[0]

positive = tweets_df[tweets_df['label']==0]
positive

negative = tweets_df[tweets_df['label']==1]
negative

sentences =tweets_df['tweet'].tolist()
sentences

len(sentences)

sentences_as_one_string =" ".join(sentences)

"""**word cloud install using pip**

"""

!pip install WordCloud
from wordcloud import WordCloud

plt.figure(figsize=(20,20))
plt.imshow(WordCloud().generate(sentences_as_one_string))

negative_list = negative['tweet'].tolist()
negative_list
negative_sentences_as_one_string = " ".join(negative_list)
plt.figure(figsize=(20,20))
plt.imshow(WordCloud().generate(negative_sentences_as_one_string))

import string
string.punctuation

test='goood morning, baby {love}#!@@@$%#%A%%A^#$'

test_punc_removed =[ char for char in  test if char not in string.punctuation ]

test_punc_removed

test_punc_removed_join = ''.join(test_punc_removed)
test_punc_removed_join

test_punc_removed = []
for char in test: 
    if char not in string.punctuation:
        test_punc_removed.append(char)

# Join the characters again to form the string.
test_punc_removed_join = ''.join(test_punc_removed)
test_punc_removed_join

import nltk # Natural Language tool kit 

nltk.download('stopwords')

from nltk.corpus import stopwords
stopwords.words('english')

test_punc_removed_join_clean=[word for word in test_punc_removed_join.split() if word.lower() not in stopwords.words('english')]
test_punc_removed_join_clean

mini_challenge = 'Here is a mini challenge, that will teach you how to remove stopwords and punctuations!'

challege = [ char     for char in mini_challenge  if char not in string.punctuation ]
challenge = ''.join(challege)
challenge = [  word for word in challenge.split() if word.lower() not in stopwords.words('english')  ]

challenge

from sklearn.feature_extraction.text import CountVectorizer
sample_data = ['This is the first paper.','This document is the second paper.','And this is the third one.','Is this the first paper?']

vectorizer= CountVectorizer()
x=vectorizer.fit_transform(sample_data)
print(vectorizer.get_feature_names())

print(x)

#Tokkeniztion
print(x.toarray())

mini_challenge = ['Hello World','Hello Hello World','Hello World world world']

vectorizer= CountVectorizer()
x=vectorizer.fit_transform(mini_challenge)
print(vectorizer.get_feature_names())

print(x.toarray())

"""**CREATE A PIPELINE TO REMOVE PUNCTUATIONS, STOPWORDS AND PERFORM COUNT VECTORIZATION**"""

def message_cleaning(message):
    test_punc_removed = [char for char in message if char not in string.punctuation]
    test_punc_removed_join = ''.join(test_punc_removed)
    test_punc_removed_join_clean = [word for word in test_punc_removed_join.split() if word.lower() not in stopwords.words('english')]
    return test_punc_removed_join_clean

# Let's test the newly added function 
tweets_df_clean = tweets_df['tweet'].apply(message_cleaning)

print(tweets_df_clean[5]) # show the cleaned up version

print(tweets_df['tweet'][5]) # show the original version

from sklearn.feature_extraction.text import CountVectorizer
# Define the cleaning pipeline we defined earlier
vectorizer = CountVectorizer(analyzer = message_cleaning, dtype = 'uint8')
tweets_countvectorizer = vectorizer.fit_transform(tweets_df['tweet'])

print(vectorizer.get_feature_names())

print(tweets_countvectorizer.toarray())

tweets_countvectorizer.shape

tweets = pd.DataFrame(tweets_countvectorizer.toarray())

X = tweets

x

y = tweets_df['label']

x=tweets_countvectorizer

x.shape

y.shape

import sklearn

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y, test_size=0.2)

from sklearn.naive_bayes import MultinomialNB
NB_classifier =MultinomialNB()
NB_classifier.fit(x_train ,y_train)

from sklearn.metrics import classification_report, confusion_matrix

# Predicting the Test set results
y_predict_test = NB_classifier.predict(x_test)
cm = confusion_matrix(y_test, y_predict_test)
sns.heatmap(cm, annot=True)

print(classification_report(y_test, y_predict_test))





